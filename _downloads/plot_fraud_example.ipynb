{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n============================================\nexample: detecting fraud on transaction data\n============================================\n\nAn example using FraudToRules for finding good rules in a fraud detection\nsetting.\n\nFraudToRules find logical rules with high precision and fuse them. Finding good\nrules is done by fitting classification and regression trees to sub-samples.\nA fitted tree defines a set of rules (each tree node defines a rule); rules\nare then tested out of the bag, and the ones with higher precision are selected\nand merged. This produces a integer-valued decision function, reflecting for\neach new samples how many rules have find it abnormal.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(__doc__)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom ftrules import FraudToRules\n\nrng = np.random.RandomState(42)\n\n# data\ndata = pd.read_csv('dataset.csv')\n\ntarget = data['isFraude'].values\ndel data['isFraude']\ndata = pd.get_dummies(data, columns=['accepteur', 'securizedTransaction'])\n\n# rm * in binary col name\nfeature_names = list(data.columns)\nfeature_names[4] = 'accepteur_ZBBIOHSD'\ndata.columns = feature_names\nprint(feature_names)\ndata = data.values\n\nn_samples = data.shape[0]\nn_samples_train = int(n_samples / 2)\ny_train = target[:n_samples_train]\ny_test = target[n_samples_train:]\nX_train = data[:n_samples_train]\nX_test = data[n_samples_train:]\n\n# fit the model\nclf = FraudToRules(max_depth=2, max_features=0.5, max_samples_features=0.5,\n                   random_state=rng, n_estimators=10,\n                   feature_names=feature_names)\nclf.fit(X_train, y_train)\nRF = RandomForestClassifier()\nRF.fit(X_train, y_train)\n\nscoring = clf.decision_function(X_test)\n\nscoring_RF = RF.predict_proba(X_test)[:, 1]\nscoring_one_rule = np.zeros(X_test.shape[0])\nrule = clf.rules_[0][0]\ndetected_index = list(\n    pd.DataFrame(X_test, columns=feature_names).query(rule).index)\nscoring_one_rule[detected_index] = 1\nprint('best rule precision:', y_test[detected_index].mean())\n\n# XXX add semi-weighted PR in some utils dir and plot it too?\nfig, axes = plt.subplots(2, 2, figsize=(20, 10),\n                         sharex=True, sharey=True)\n\ncurves = [roc_curve, precision_recall_curve] * 2\nscores = [scoring] * 2 + [scoring_one_rule] * 2\nxlabels = ['FPR', 'Recall'] * 2\nylabels = ['TPR', 'Precision'] * 2\n\nfor ax, curve, score, xlabel, ylabel in zip(axes.flatten(), curves, scores,\n                                            xlabels, ylabels):\n    x, y, _ = curve(y_test, score)\n    auc_score = auc(y, x)\n    label = ('AUC: %0.3f' % auc_score)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.plot(x, y, lw=1, label=label)\n\n    x_rf, y_rf, _ = curve(y_test, scoring_RF)\n    ax.plot(x_rf, y_rf, '-.', c='g',  label='RF')\n\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}